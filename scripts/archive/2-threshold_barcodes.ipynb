{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USER INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Experiment name to prepend output files\"\"\"\n",
    "EXPERIMENT = \"sample2\"\n",
    "\n",
    "\"\"\"Directory path to input data \n",
    "    (filtered; output from '1-read_fastq_id_features)\n",
    "\"\"\"\n",
    "FILTERED_FILEPATH = \"../output/filtered-sample2.csv\"\n",
    "FILTERED_FILEPATH = \"../output/filtered-sample.csv\"\n",
    "\n",
    "\"\"\"Directory path to save output\"\"\"\n",
    "OUTPUT_DIRECTORY = \"../output\"\n",
    "\n",
    "\"\"\"Minimum number of reads as a baseline. Used to simplify data processing.\"\"\"\n",
    "MIN_READS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRESETS AND SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Column names correspoinding to eponymous variables\"\"\"\n",
    "\n",
    "SAMPLE = 'idx'\n",
    "QTAG = 'qtag'\n",
    "BARCODE = 'barcode'\n",
    "READS = 'readsPF'\n",
    "MCOUNTS = 'mcountsPF'\n",
    "PERCENT_MCOUNTS = 'percent_%s'%MCOUNTS\n",
    "GROUPBY = ['idx']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_inputs():\n",
    "    \n",
    "    # modules\n",
    "    assert pd\n",
    "    assert np\n",
    "    assert regex\n",
    "    assert os\n",
    "    assert sys\n",
    "    \n",
    "    # user experiment inputs \n",
    "    assert EXPERIMENT\n",
    "    assert FILTERED_FILEPATH\n",
    "    assert OUTPUT_DIRECTORY\n",
    "    assert MIN_READS\n",
    "    \n",
    "    # user columns\n",
    "    assert SAMPLE\n",
    "    assert QTAG\n",
    "    assert BARCODE\n",
    "    assert READS\n",
    "    assert MCOUNTS\n",
    "    assert PERCENT_MCOUNTS\n",
    "    assert GROUPBY\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_percent_molecs(df, mcounts=MCOUNTS, percent_mcounts=PERCENT_MCOUNTS):\n",
    "    \"\"\"Updates percent molec counters for sample\"\"\"\n",
    "    total = float(df[mcounts].values.sum()) / 100.\n",
    "    df[percent_mcounts] = df[mcounts].apply(lambda x: x/total)\n",
    "    df = df.sort_values(by=mcounts, ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filtered_file, sample=SAMPLE, qtag=QTAG, \n",
    "              barcode=BARCODE, mcounts=MCOUNTS, reads=READS):\n",
    "    \"\"\"Loads filtered lib-ID-barcode data csv to dict of samples\"\"\"\n",
    "    columns = [sample, qtag, barcode, mcounts, reads]\n",
    "    # loads excel file (all tabs)\n",
    "    csv = pd.read_csv(filtered_file)\n",
    "    # filter out null barcodes just in case (if custom user input)\n",
    "    csv = csv.loc[(csv[qtag]!='None') & (csv[barcode]!='None')]\n",
    "    csv = csv[columns]\n",
    "    csv[sample] = csv[sample].apply(lambda x: str(x))\n",
    "    # get percent molecs per sample, store as output dict entry \n",
    "    groups = csv.groupby(sample)\n",
    "    data = []\n",
    "    for i, group in csv.groupby(sample):\n",
    "        data.append((i,calculate_percent_molecs(group)))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_data(d, sample=SAMPLE, mcounts=MCOUNTS, reads=READS):\n",
    "    \"\"\"Check data for proper format, input values, and \n",
    "    converts into list-like object if necessary\n",
    "\n",
    "    d(list, np.array ,dict, or pd.DataFrame): input data\n",
    "    \n",
    "    Returns: data set as a list-like object, wherein\n",
    "        each item is a pair containing sample name (str) and \n",
    "        sample data (pd.DataFrame), in that order.\n",
    "    \"\"\"\n",
    "    data_arr = []\n",
    "    # wrangle data to list of list-like pairs, as \"[idx, df]\"\n",
    "    if type(d) in [np.array, list] :\n",
    "        data_arr = d\n",
    "    elif type(d) == dict:\n",
    "        data_arr = d.items()\n",
    "    elif type(d) == pd.DataFrame :\n",
    "        data_arr = [(s,df) for s,df in d.groupby(sample)]\n",
    "    else:\n",
    "        print \"Input data is not in correct format. Please provide \\\n",
    "        list-like, dict, or pd.DataFrame object.\"\n",
    "    \n",
    "    # check input has correct values\n",
    "    try:\n",
    "        for a in data_arr:\n",
    "            assert len(a) == 2, \"incorrect item length\"\n",
    "            s, df = a\n",
    "            assert type(s) == str, 'sample name is not string type'\n",
    "            assert type(df) == pd.DataFrame, \"incorrect value type: must be pd.DataFrame\"\n",
    "            assert sample in df.columns, \"%s not in dataframe\"%sample\n",
    "            assert mcounts in df.columns, \"%s not in dataframe\"%mcounts\n",
    "            assert reads in df.columns, \"%s not in dataframe\"%reads\n",
    "    # if no \n",
    "    except IndexError as e:\n",
    "        print \"Item number of values is not 2.\\n\"\n",
    "        print \"IndexError. \",e.message\n",
    "        print a\n",
    "    except ValueError as e:\n",
    "        print \"Sample name could not be converted to float: %s\\n\"% type(item[i])\n",
    "        print \"ValueError. \",e.message\n",
    "        print a\n",
    "    except AssertionError as e:\n",
    "        print \"Assertion failed:\"\n",
    "        print e.message\n",
    "        print a\n",
    "    \n",
    "    return data_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def threshold(group, reps_remaining, thresh_val, thresh_i,\n",
    "             percent_mcounts=PERCENT_MCOUNTS, mcounts=MCOUNTS):\n",
    "    \"\"\"Thresholds barcodes of a given sample\n",
    "\n",
    "        group(pd.DataFrame): df containing library-ID-barcodes, \n",
    "            mcountsPF and percent_mcountsPF\n",
    "        reps_remaining(int): reps remaining from max number \n",
    "            input from user \n",
    "        thresh_val(float or int): initial threshold value (percent_mcountPF)\n",
    "            provided from previous recursion or user input\n",
    "        thresh_i(int): initial position of threshold value in \n",
    "            percent_mcountsPF list, ranging [0,len(group))\n",
    "\n",
    "        Returns: \n",
    "            None, if thresholding fails;\n",
    "            passed(pd.DataFrame), if thresholding successful; or\n",
    "            self, otherwise, with updated threshold values and \n",
    "                group df.\n",
    "    \"\"\"\n",
    "    # max out reps\n",
    "    if reps_remaining <= 0:\n",
    "        print 'Maxed out reps. Skipping sample.'\n",
    "    # no barcodes passed threshold \n",
    "    elif len(group) == 0:\n",
    "        print \"No barcodes passed threshold. Skipping sample.\"\n",
    "    else:\n",
    "        # calculate new threshold \n",
    "        \n",
    "        # line add 2016-10-12 to pre-sort values\n",
    "        group.sort_values(by=mcounts, ascending=False, inplace=True)\n",
    "        # existing\n",
    "        group = calculate_percent_molecs(group)\n",
    "        calc_threshold_i = calculate_threshold(group[mcounts].values)\n",
    "        new_thresh_i = min(calc_threshold_i, len(group)-1)\n",
    "        new_thresh_val = group[percent_mcounts].values[new_thresh_i]\n",
    "#         print \"\\nTOP OF THRESHOLD \"\n",
    "#         print 'group len', len(group)\n",
    "#         print 'thresh_val', thresh_val\n",
    "#         print 'thresh_i', thresh_i\n",
    "#         print 'calc_threshold_i', calc_threshold_i\n",
    "#         print 'new_thresh_i', new_thresh_i\n",
    "#         print 'new_thresh_val',new_thresh_val, '\\n'\n",
    "        \n",
    "        # if reached steady state\n",
    "        if new_thresh_val == thresh_val:\n",
    "            # get rid of any \"padding\" barcodes (see eliminate_oneoffs fn)\n",
    "            # line added 2016-10-12 to remove barcodes that didn't pass threshold\n",
    "            passed_temp = group.loc[(group.is_padding==False) & (group[percent_mcounts]>=new_thresh_val)]\n",
    "            \n",
    "            passed = calculate_percent_molecs(passed_temp)\n",
    "            # update percent molecs\n",
    "            passed.reset_index(inplace=True,drop=True)\n",
    "#             print 'barcodes', len(group)\n",
    "            sys.stdout.write('Thresholded.\\n')\n",
    "                       \n",
    "            return passed\n",
    "        # recursively clean and re-threshold\n",
    "        else:\n",
    "            # clean up group by eliminating one-offs\n",
    "            \n",
    "            group = calculate_percent_molecs(group)\n",
    "            cleaned = eliminate_oneoffs(group,new_thresh_val)\n",
    "            cleaned.reset_index(inplace=True,drop=True)\n",
    "            # recurse with cleaned df and new threshold values\n",
    "            return threshold(cleaned, reps_remaining-1, new_thresh_val, \n",
    "                             new_thresh_i, percent_mcounts=percent_mcounts, \n",
    "                             mcounts=mcounts)\n",
    "        \n",
    "    # if thresholding failed, return None\n",
    "    sys.stdout.write('Skipped.\\n')\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP 2: CALCULATE THRESHOLD via. CONCAVITY\n",
    "\n",
    "\n",
    "def calculate_threshold(y):\n",
    "    \"\"\"Calculates threshold of series with modified concavity approach\n",
    "\n",
    "        y(np.array or list): list or list-like object of \n",
    "            values as floats or ints\n",
    "\n",
    "        Returns index of inflection point in array, \n",
    "            i.e. threshold position.\n",
    "    \"\"\"\n",
    "    def rolling_window(arr):\n",
    "        \"\"\"Constructs list of overlapping subarray ranges of size 2\"\"\"\n",
    "        shape = arr.shape[:-1] + (arr.shape[-1]-1, 2)\n",
    "        strides = arr.strides + (arr.strides[-1],)\n",
    "        windows = np.lib.stride_tricks.as_strided(arr, \n",
    "                              shape=shape, strides=strides)\n",
    "        return windows\n",
    "    def first_d_gen(windows):\n",
    "        \"\"\"Generates first derivative of windows as relative difference\"\"\"\n",
    "        for w in windows:\n",
    "            # amended 2016-10-12: normalize by y midpoint instead of second point to \n",
    "            # better represent the count magnitude of segment\n",
    "            yield float(w[1]-w[0])/(w[0]+w[1])*2\n",
    "    def second_d_gen(windows):\n",
    "        \"\"\"Generates second derivative of windows\"\"\"\n",
    "        for w in windows:\n",
    "            yield w[1]-w[0]         \n",
    "    \n",
    "    y_temp = sorted(y, reverse=True)\n",
    "#     if False in [yi==y_tempi for yi, y_tempi in zip(y, y_temp)]:\n",
    "#         print \"DIFFERENT!\"\n",
    "    # left and right padding to cover all array vals in derivations\n",
    "    yarray = np.concatenate([ [y_temp[0]], y_temp, [1] ])\n",
    "    # calculates first derivative\n",
    "    first_windows = rolling_window(yarray)\n",
    "    first_derivs = np.fromiter(first_d_gen(first_windows), np.float\n",
    "                               , count=len(first_windows))\n",
    "    # calculates second derivative\n",
    "    second_windows = rolling_window(first_derivs)\n",
    "    second_derivs = np.fromiter(second_d_gen(second_windows), np.float\n",
    "                                , count=len(second_windows))\n",
    "    # gets index or position value of inflection point (curves down ), adjust by adding 1\n",
    "    # for second deriv\n",
    "    thresh_i = np.argmin(second_derivs)+1\n",
    "    \n",
    "#     print \"\\nCALCULATE THRESHOLD\"\n",
    "#     print \"2nd max\", np.argmax(second_derivs), np.max(second_derivs)\n",
    "#     print \"2nd min\", np.argmin(second_derivs), np.min(second_derivs)\n",
    "#     print \"\\n\"\n",
    "    return thresh_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eliminate_oneoffs(group, thresh_val, pad=True,\n",
    "                      qtag=QTAG, barcode=BARCODE, percent_mcounts=PERCENT_MCOUNTS, \n",
    "                      mcounts=MCOUNTS):\n",
    "    \"\"\"Eliminate barcodes that are one position off from a more-abundant barcode\n",
    "        group(pd.DataFrame): df containing qtag, barcode, and percent_mcounts columns\n",
    "        thresh_val(float): threshold value to select high abundant barcodes\n",
    "            to iterate through as 'major' ones\n",
    "        pad(bool): if True, adds a right pad so last non-eliminated value \n",
    "            can be analyzed in by the threshold() function. Default True.\n",
    "        qtag, barcode, percent_mcounts, mcounts (str): column names for the corresponding\n",
    "            argument. Defaults are global vars QTAG, BARCODE, PERCENT_MCOUNTS, MCOUNTS.\n",
    "\n",
    "        Returns: table of barcodes that passed elimination\n",
    "    \"\"\"\n",
    "    group.loc[:,'delete'] = group[mcounts].apply(lambda _: False)\n",
    "    group.loc[:,'is_padding'] = group[mcounts].apply(lambda _: False)\n",
    "    counter = 0\n",
    "    # add capability to check other parameters, i.e. qtag\n",
    "    for majorI, majorRow in group.loc[group[percent_mcounts] > thresh_val].iterrows():\n",
    "        # if it has not yet been tested\n",
    "        if majorRow.delete == False:\n",
    "            subgroup = group[counter+1:].loc[(group.delete==False)]\n",
    "            # for each 'minor' barcode aka. with fewer molecs, test if one-off from major\n",
    "            for minorI, minorRow in subgroup.iterrows():\n",
    "                query = regex.search(\"(%s){s<=1}\" % majorRow[barcode],\n",
    "                                     minorRow[barcode])\n",
    "                if query:\n",
    "                    group.loc[minorI,'delete'] = True\n",
    "        counter+=1\n",
    "        \n",
    "    # select barcodes which pass, ie. are not eliminated\n",
    "    output = group.loc[(group.delete==False) & (group[percent_mcounts] >= thresh_val)]\n",
    "#     print \"\\nEND OF ELIMINATE ONEOFFS\"\n",
    "#     print len(output)\n",
    "    # if requested, adds a right pad  \n",
    "    # line added 2016-10-12: combine two conditions: ensure that \n",
    "    # no null rows get added if all barcodes are accepted\n",
    "    if pad==True and len(group) != len(group.delete==False):\n",
    "        deletes = group.loc[(group.delete==True) & (group[percent_mcounts]<thresh_val)][mcounts]\n",
    "#         if len(deletes) > 0:\n",
    "        max_i = deletes.idxmax()\n",
    "        output.append(group.loc[max_i,:])\n",
    "        output.loc[max_i,['delete', 'is_padding']] = [False, True]\n",
    "#     print output.head(), \"\\n\"\n",
    "    return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_threshold(d, sample=SAMPLE, qtag=QTAG, barcode=BARCODE, \n",
    "                  mcounts=MCOUNTS, reads=READS, percent_mcounts=PERCENT_MCOUNTS,\n",
    "                  min_reads=MIN_READS, min_mcount=50):\n",
    "    \"\"\"Run threshold algorithm for each sample in dataset\n",
    "\n",
    "        d(np.array, list, dict, or pd.DataFrame): dataset for all samples\n",
    "        sample, qtag, barcode, mcounts, reads, percent_mcounts(str): columns in df for \n",
    "            corresponding vals. Defaults are global vars SAMPLE, QTAG, BARCODE, MCOUNTS, \n",
    "            READS, PERCENT_MCOUNTS.\n",
    "        min_reads(int): minimum number of reads for a library-ID-barcode as an \n",
    "            absolute baseline (that which any barcode below is highly likely to be \n",
    "            false.) Default is global var MIN_READS.\n",
    "        min_mcount(int): minimum number of molecs for library-ID-barcode as an\n",
    "            absolute baseline. Default is 50.\n",
    "\n",
    "        Note: min_reads and min_mcount are applied to increase performance.\n",
    "\n",
    "        Returns: \n",
    "            pd.DataFrame, if successful, of 'true' (passed) library-ID-barcodes \n",
    "                for all samples\n",
    "            None, if no samples had passing library-ID-barcodes.\n",
    "    \"\"\"\n",
    "    passed = []\n",
    "    counter = 1\n",
    "    # checks and formats data (d) to list-like obj of pairs\n",
    "    data_arr = check_data(d, sample=sample, mcounts=mcounts, reads=reads)\n",
    "    # run for each (sample, df) in dataset \n",
    "    for s, group in data_arr:\n",
    "#         print '\\n\\n>>>>> NEW INDEX'\n",
    "        sys.stdout.write(\"Sample %d of %d (%s): \"%(counter,len(data_arr),s))\n",
    "        # select valid data meeting absolute baseline \n",
    "        group = group.loc[(group[qtag] != 'None') & (group[barcode] != 'None') \n",
    "                          & (group[mcounts] > min_mcount)\n",
    "                          & (group[reads] > min_reads)]\n",
    "        result = threshold(group, 20, -1, len(group)+2)\n",
    "        passed.append(result)\n",
    "        sys.stdout.flush()\n",
    "        counter += 1\n",
    "    # if we do have data (i.e. some barcodes that passed in the samples)\n",
    "    if len(passed) > 0:\n",
    "        # concat all df together\n",
    "        passeddf = pd.concat(passed)\n",
    "        # formatting\n",
    "        passeddf.sort_values(by=[sample,percent_mcounts]\n",
    "                             ,ascending=[True, False]\n",
    "                             ,inplace=True)\n",
    "        passeddf.drop(['delete','is_padding'], axis=1, inplace=True)\n",
    "        passeddf.reset_index(inplace=True,drop=True)\n",
    "        return passeddf\n",
    "    else:\n",
    "        print \"No samples were successfully thresholded.\"\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRIPT TO SAVE DATA TO CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_data(filtered,passed, output_directory=OUTPUT_DIRECTORY, experiment=EXPERIMENT):\n",
    "    \n",
    "    # Save passed dataframe\n",
    "    passed.to_csv('%s/threshold-%s-passed.csv'%(output_directory,experiment), index=False)\n",
    "    # count and save results\n",
    "    counts = count_sample_barcodes(passed)\n",
    "    counts.to_csv('%s/threshold-%s-counts.csv'%(output_directory,experiment), index=False)\n",
    "    # merge passed and filtered data and save\n",
    "    fc_temp = pd.concat([d[1] for d in filtered])\n",
    "    filtered_concat = apply_passed_data(fc_temp, passed)\n",
    "    filtered_concat.to_csv('%s/threshold-%s-merged.csv'%(output_directory,experiment), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_sample_barcodes(df, groupby=SAMPLE, barcode=BARCODE, sample=SAMPLE, qtag=QTAG\n",
    "                      , percent_mcounts=PERCENT_MCOUNTS):\n",
    "    \"\"\"Counts passed barcodes per sample and saves as csv\n",
    "    \n",
    "        df(pd.DataFrame): input dataframe containing passed \n",
    "            barcodes of all samples\n",
    "        groupby(str or list-like): columns to group samples by\n",
    "    \"\"\"\n",
    "    agg = df.groupby(groupby).agg(len)\n",
    "    counts = agg[agg.columns[0]]\n",
    "    counts.name = 'count'\n",
    "    counts = pd.DataFrame(counts)\n",
    "    counts.reset_index(inplace=True)\n",
    "    return counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_passed_data(filtered, passed, sample=SAMPLE, qtag=QTAG\n",
    "                      , barcode=BARCODE, percent_mcounts=PERCENT_MCOUNTS):\n",
    "    \"\"\"Cross reference passed barcodes with raw filtered data\n",
    "        filtered(pd.DataFrame): raw filtered data as dataframe for all samples\n",
    "        passed(pd.DataFrame): data passed threshold (output of run_threshold)\n",
    "        sample,qtag,barcode,percent_mcounts(str): names corresponding to df columns.\n",
    "            Defaults are SAMPLE,QTAG,BARCODE,PERCENT_MCOUNTS.\n",
    "\n",
    "        Returns:\n",
    "            filtered(pd.DataFrame) updated with 'passed_threshold' and \n",
    "            percent_mcounts columns\n",
    "    \"\"\"\n",
    "    def cross_ref_passed(row):\n",
    "        key = (row[sample],row[qtag],row[barcode])\n",
    "        if key in passed_indexed.index:\n",
    "            row[percent_mcounts+'_thresholded'] = passed_indexed.loc[key,percent_mcounts]\n",
    "            row['passed_threshold'] = True\n",
    "        return row\n",
    "\n",
    "    passed_indexed = passed.set_index([sample,qtag,barcode])\n",
    "    filtered.rename(columns={percent_mcounts:percent_mcounts+'_filtered'})\n",
    "    filtered['passed_threshold'] = False\n",
    "    filtered[percent_mcounts+'_thresholded'] = 0\n",
    "    filtered = filtered.apply(cross_ref_passed, axis=1)\n",
    "    return filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXECUTE SCRIPT BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    # Check all inputs exist and are valid\n",
    "    check_inputs()\n",
    "    # Runs data loading in script\n",
    "    filtered = load_data(FILTERED_FILEPATH)\n",
    "    # Run thresholding of all samples in dataset\n",
    "    passed = run_threshold(filtered)\n",
    "    save_data(filtered, passed, output_directory=OUTPUT_DIRECTORY, experiment=EXPERIMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print passed\n",
    "# print passed.groupby('idx').apply(len)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
